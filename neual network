import numpy as np
def sigmoid(x):
    s = 1/(1+np.exp(-x))
    return s
def init_W():
    
    n = input('input dim:')
    M = input('output dim:')
    np.random.seed(3)
    W=np.random.randn(int(M),int(n))*.01
    #b = np.zeros((int(M),1))*.01
    return W,int(M)
def init_b(M):
    b = np.zeros((int(M),1))*.01
    return b

def nueral_network(X,Y,I,N,learningrate):
    m = X.shape[1]
    #np.random.seed(2)
    for x in range(1, N+1):
        globals()['W%s' % x],M =init_W()
        globals()['b%s' % x] = init_b(M)
    for i in range(0,I):
### Foward popagation
        for i in range(1,N+1):
            if i ==1:
                globals()['Z%s' % int(i)]=np.dot(globals()['W%s' % int(i)],X)+globals()['b%s' % int(i)]
                globals()['A%s' % int(i)]=np.tanh(globals()['Z%s' % int(i)])
            if i == N:
                globals()['Z%s' % int(i)]=np.dot(globals()['W%s' % int(i)],globals()['A%s' % int(i-1)])+globals()['b%s' % int(i)]
                globals()['A%s' % int(i)]=sigmoid(globals()['Z%s' % int(i)])                
            if (i!=1) and (i!=N):
                globals()['Z%s' % int(i)]=np.dot(globals()['W%s' % int(i)],globals()['A%s' % int(i-1)])+globals()['b%s' % int(i)]
                globals()['A%s' % int(i)]=np.tanh(globals()['Z%s' % int(i)])
### Backward popagation
        for i in range(-N,0):
            if i == -int(N):
                globals()['dz%s' % -i]=globals()['A%s' % -i] -Y
                globals()['dW%s' % -i]= (1/m)*(np.dot(globals()['dz%s' % -i],globals()['A%s' % -(i+1)].T))
                globals()['db%s' % -i]= (1/m)*(np.sum(globals()['dz%s' % -i], axis=1, keepdims=True))
            if i==-1:
                globals()['dz%s' % -i] = (np.dot(globals()['W%s' % -(i-1)].T,globals()['dz%s' % -(i-1)]))*(1-np.power(globals()['A%s' % -i],2))
                globals()['dW%s' % -i]= (1/m)*(np.dot(globals()['dz%s' % -i],X.T))
                globals()['db%s' % -i]= (1/m)*(np.sum(globals()['dz%s' % -i], axis=1, keepdims=True))        
            if (i!=-1) and (i!=-N):
                globals()['dz%s' % -i] = (np.dot(globals()['W%s' % -(i-1)].T,globals()['dz%s' % -(i-1)]))*(1-np.power(globals()['A%s' % -i],2))
                globals()['dW%s' % -i]= (1/m)*(np.dot(globals()['dz%s' % -i],globals()['A%s' % -(i+1)].T))
                globals()['db%s' % -i]= (1/m)*(np.sum(globals()['dz%s' % -i], axis=1, keepdims=True))      
### weight updates
        for i in range(1,N+1):
                globals()['W%s' % i]=globals()['W%s' % i]-learningrate*globals()['dW%s' % i]
                globals()['b%s' % i]=globals()['b%s' % i]-learningrate*globals()['db%s' % i]

    Cost=(-1/m)*np.sum((Y*np.log(globals()['A%s' % N]))+((1-Y)*np.log(1-globals()['A%s' % N])))
    print(Cost)
    return globals()['A%s' % N]

yp=nueral_network(X,Y,30000,3
                  ,1.2)
